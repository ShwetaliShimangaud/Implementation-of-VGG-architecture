{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip  '/content/drive/MyDrive/imagenette2-320.zip'"
      ],
      "metadata": {
        "id": "F2qoGkSanTQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ra38ZknpmNM_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def read_dataset(path, size):\n",
        "    # Define image data generator with augmentation options if needed\n",
        "    image_generator = ImageDataGenerator()\n",
        "\n",
        "    # Set up the image generator to flow from the directory\n",
        "    batch_size = 32\n",
        "    image_size = (size, size)  # adjust the size as needed\n",
        "    class_mode = 'categorical'  # or 'binary' if it's a binary classification\n",
        "\n",
        "    # Flow images in batches from the directory\n",
        "    data_generator = image_generator.flow_from_directory(\n",
        "        path,\n",
        "        target_size=image_size,\n",
        "        batch_size=batch_size,\n",
        "        class_mode=class_mode\n",
        "    )\n",
        "    return data_generator\n",
        "\n",
        "\n",
        "def get_train_dataset(size):\n",
        "    path = '/content/imagenette2-320/train'\n",
        "    return read_dataset(path, size)\n",
        "\n",
        "\n",
        "def get_validation_dataset(size):\n",
        "    path = '/content/imagenette2-320/val'\n",
        "    return read_dataset(path, size)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import dataset_loader\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import top_k_accuracy_score\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D"
      ],
      "metadata": {
        "id": "zampNEFn2oXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "  model = Sequential()\n",
        "  model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu',kernel_initializer='uniform'))\n",
        "  model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
        "  model.add(Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
        "  model.add(Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
        "  model.add(Conv2D(256, (1, 1), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
        "  model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
        "  model.add(Conv2D(512, (1, 1), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
        "  model.add(Conv2D(512, (3, 3), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
        "  model.add(Conv2D(512, (1, 1), strides=(1, 1), padding='same', activation='relu', kernel_initializer='uniform'))\n",
        "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(Conv2D(4096, (7, 7), activation='relu', padding='same'))\n",
        "  model.add(Dropout(rate=0.5))\n",
        "  model.add(Conv2D(4096, (1, 1), activation='relu', padding='same'))\n",
        "  model.add(Dropout(rate=0.5))\n",
        "  model.add(Conv2D(10, (1, 1), activation='softmax'))\n",
        "  model.add(tf.keras.layers.GlobalMaxPooling2D())\n",
        "  model.add(Flatten())\n",
        "\n",
        "\n",
        "# Create a new model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=1e-5), metrics=[keras.metrics.TopKCategoricalAccuracy(k=1, name='top 1%'), keras.metrics.TopKCategoricalAccuracy(k=5, name='top 5%')])\n",
        "  return model\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "ZUf3ujSK1uyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of label classes\n",
        "\n",
        "num_classes = 10\n",
        "epochs = 25\n",
        "\n",
        "train_data = get_train_dataset(256)\n",
        "\n",
        "# Experiment 1.1 : S=256 Q = 224\n",
        "test_data_224 = get_validation_dataset(224)\n",
        "score_224 = get_model().fit(train_data,validation_data=test_data_224, epochs=epochs)\n",
        "\n",
        "\n",
        "# Experiment 1.2 : S=256 Q = 256\n",
        "test_data_256 = get_validation_dataset(256)\n",
        "score_256 = get_model().fit(train_data,validation_data=test_data_256, epochs=epochs)\n",
        "\n",
        "# Experiment 1.3 : S=256 Q = 288\n",
        "test_data_288 = get_validation_dataset(288)\n",
        "score_288 = get_model().fit(train_data,validation_data=test_data_288, epochs=epochs)"
      ],
      "metadata": {
        "id": "vxRWUHL-nNCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_384 = get_train_dataset(384)\n",
        "\n",
        "# Experiment 1.1 : S=384 Q = 352\n",
        "test_data_352 = get_validation_dataset(352)\n",
        "score_352 = get_model().fit(train_data_384,validation_data=test_data_352, epochs=epochs)\n",
        "\n",
        "\n",
        "# Experiment 1.2 : S=384 Q = 384\n",
        "test_data_384 = get_validation_dataset(384)\n",
        "score_384 = get_model().fit(train_data_384,validation_data=test_data_384, epochs=epochs)\n",
        "\n"
      ],
      "metadata": {
        "id": "EWxNi52p41gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_320 = get_train_dataset(320)\n",
        "\n",
        "# Experiment 1.1 : S=320 Q = 256\n",
        "score_256_320 = get_model().fit(train_data_320,validation_data=test_data_256, epochs=epochs)\n",
        "\n",
        "\n",
        "# Experiment 1.2 : S=320 Q = 320\n",
        "test_data_320 = get_validation_dataset(320)\n",
        "score_320 = get_model().fit(train_data_320,validation_data=test_data_320, epochs=epochs)\n",
        "\n",
        "# Experiment 1.3 : S=320 Q = 384\n",
        "score_384_320 = get_model().fit(train_data_320,validation_data=test_data_384, epochs=epochs)"
      ],
      "metadata": {
        "id": "ZZ-TfCKX5Lin"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}